---
title: 'Homework 3: Samplers'
author: "Ben Pelczynski"
date: "2024-11-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1:

```{r}
#Make sure you install the Rmpfr library
library(Rmpfr)
dat = read.csv("coaldisasters-ds6040.csv")


gibbs_sampler = function(iter, dat, a_mu, b_mu, a_lambda, b_lambda){
  
  mu_vec = vector()
  lambda_vec = vector() 
  k_prob_mat = matrix(nrow = iter+1, ncol = 111)
  k_samp_vec = vector()
  #Initialize sampler
    mu_vec[1] = rgamma(1,a_mu, rate  = b_mu)
  lambda_vec[1] = rgamma(1,a_lambda, rate = b_lambda)
  k_prob_mat[1,] = rep(1/111, 111)
  k_samp_vec[1] = 56
  
  #Sampler
  for(i in 2:(iter+1)){
    mu_vec[i] = rgamma(1, shape = a_mu + sum(dat[1:k_samp_vec[i-1], "Count"]),
                       rate = k_samp_vec[i-1] + b_mu)
    lambda_vec[i] = rgamma(1, shape = a_lambda + sum(dat[(k_samp_vec[i-1]+1):112, "Count"]),
                       rate = 112 - k_samp_vec[i-1] + b_lambda)
    
    l_temp = vector()
  for(j in 1:111){  
    l_temp[j] = sum(log(mpfr(dpois(dat[1:j,2], lambda = rep(mu_vec[i],j)), precBits = 100))) + sum(log(mpfr(dpois(dat[(j+1):112,2], lambda = rep(lambda_vec[i],112-j)), precBits = 100)))
  }
  l_temp <- mpfr(l_temp, precBits = 100)
  k_prob_mat[i,] = as.numeric(exp(l_temp)/sum(exp(l_temp))) 
  k_samp_vec[i] = sample(size = 1,1:111, prob = k_prob_mat[i,])
  }
  toReturn = data.frame(mu = mu_vec, lambda = lambda_vec, k = k_samp_vec)
  
  return(toReturn)
}

test = gibbs_sampler(1000, dat, a_mu = 1, b_mu = 1, a_lambda = 1, b_lambda = 1)
```

```{r}
library(ggplot2)
library(dplyr)

ggplot(test) + geom_density(aes(x = mu), color='red') + geom_density(aes(x = lambda), color='blue') + xlab('mu (red) & lambda (blue)')

mu_EAP = mean(test$mu)
lambda_EAP = mean(test$lambda)
mu_CI= quantile(test$mu, probs = c(0.025, 0.975))
lambda_CI = quantile(test$lambda, probs = c(0.025, 0.975))
tibble(mu_EAP, mu_CI, lambda_EAP, lambda_CI)

test %>% count(k) %>% arrange(desc(n)) %>% head(5)
```

### (a)

The EAP and credible intervals of $\mu$ and $\lambda$ suggest that, at the changepoint the rate of coal mining disasters changes from approximately 3.07 (with a variation of about 0.57) to a rate of approximately 0.92 (with a variation of about 0.23). Subsequently, the number of coal mining disasters shifts from a distribution of Poisson(\~3.07) to a distribution of Poisson(\~0.92), and thus *something* happened at the changepoint to greatly decrease the amount of mining disasters. Additionally, the lower variance of $\lambda$ suggests that the *something* was not only able to lower the rate of disasters, it was able to do so consistently. The most likely year of the changepoint is $1850 + [k = 41] = 1891$.

### (b)

The EAPs/CIs aren't necessarily the most appropriate to report for the year of the changepoint because the year of the changepoint is the year of the changepoint is inherently based on the EAPs/CIs of the $\mu$ and $\lambda$, so reporting them both is redundant.

# Part 2:

### 1.

```{r}
winedat = read.csv("whitewine-training-ds6040-1.csv")
winedat = winedat %>% mutate(ZO_quality = ifelse(winedat$wine_quality == 'A', 1, 0))
```

### 2.

```{r}
library(brms)
library(parameters)
library(see)

full_fit = brm(wine_quality ~ . -ZO_quality, data = winedat, family = categorical(), refresh = 0)
summary(full_fit)
```

```{r}
full_fitA = brm(ZO_quality ~ . -wine_quality, data = winedat, family = bernoulli(), refresh = 0)
summary(full_fitA)
```

The variables that will give the best classification rate are the variables with the largest coefficient magnitudes, as those will have the most influence on the classification model itself. Therefore, the best variables for classification rate of A-rated wines, as well as for wine overall, are residual.sugar, density, and pH.

```{r}
full_fit = brm(wine_quality ~ residual.sugar+density+pH, data = winedat, family = categorical(), refresh = 0)
plot(full_fit)
plot(model_parameters(full_fit, effects = 'fixed'))
summary(full_fit)
```

```{r}
full_fitA = brm(ZO_quality ~ residual.sugar+density+pH, data = winedat, family = bernoulli(), refresh = 0)
plot(full_fitA)
plot(model_parameters(full_fitA, effects = 'fixed'))
summary(full_fitA)
```

In both cases, the parameter distributions suggest that wine is more likely to be "A"-grade quality with increased sugars (evidenced by the positive residual.sugar coefficient, and negative respective muC and muF coefficients) and decreased density and acidity (evidenced by the negative density and pH coefficients, and positive respective muC and muF coefficients). Additionally, the intercepts being negative for the "A" classifier and positive for the "C" and "F" classifiers suggests that a hypothetically perfectly neutral wine is much more likely to be graded "F" than "A", and more likely to be graded "C" than either. It is also worth pointing out that pH has a much lower variance than the other parameters, suggesting that the wines themselves do not have as great a variance in pH, and subsequently that pH is less important than the other parameters.
